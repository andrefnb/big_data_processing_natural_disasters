{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-15 13:03:00--  https://www.dropbox.com/s/b74bpkbr89l8k43/natural_disasters_BIG.csv?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.68.1, 2620:100:6024:1::a27d:4401\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.68.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/b74bpkbr89l8k43/natural_disasters_BIG.csv [following]\n",
      "--2019-11-15 13:03:01--  https://www.dropbox.com/s/raw/b74bpkbr89l8k43/natural_disasters_BIG.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucc9819570c7610595a55e17ea82.dl.dropboxusercontent.com/cd/0/inline/AsYw-bQV7AFGhJ6MKrcp1CROm7tmOb86tjiT0Hq5zvHRUkZOwI19Kp61JY8AxPd7Luhm0OnTwBxxgoWIxSLfRsk8Y5qymNCvXc1eyheuXYri1w/file# [following]\n",
      "--2019-11-15 13:03:01--  https://ucc9819570c7610595a55e17ea82.dl.dropboxusercontent.com/cd/0/inline/AsYw-bQV7AFGhJ6MKrcp1CROm7tmOb86tjiT0Hq5zvHRUkZOwI19Kp61JY8AxPd7Luhm0OnTwBxxgoWIxSLfRsk8Y5qymNCvXc1eyheuXYri1w/file\n",
      "Resolving ucc9819570c7610595a55e17ea82.dl.dropboxusercontent.com (ucc9819570c7610595a55e17ea82.dl.dropboxusercontent.com)... 162.125.68.6, 2620:100:6024:6::a27d:4406\n",
      "Connecting to ucc9819570c7610595a55e17ea82.dl.dropboxusercontent.com (ucc9819570c7610595a55e17ea82.dl.dropboxusercontent.com)|162.125.68.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 205487817 (196M) [text/plain]\n",
      "Saving to: ‘natural_disasters_BIG.csv’\n",
      "\n",
      "natural_disasters_B  16%[==>                 ]  33.29M  47.5KB/s    in 3m 45s  \n",
      "\n",
      "2019-11-15 13:06:48 (152 KB/s) - Connection closed at byte 34913195. Retrying.\n",
      "\n",
      "--2019-11-15 13:06:49--  (try: 2)  https://ucc9819570c7610595a55e17ea82.dl.dropboxusercontent.com/cd/0/inline/AsYw-bQV7AFGhJ6MKrcp1CROm7tmOb86tjiT0Hq5zvHRUkZOwI19Kp61JY8AxPd7Luhm0OnTwBxxgoWIxSLfRsk8Y5qymNCvXc1eyheuXYri1w/file\n",
      "Connecting to ucc9819570c7610595a55e17ea82.dl.dropboxusercontent.com (ucc9819570c7610595a55e17ea82.dl.dropboxusercontent.com)|162.125.68.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 PARTIAL CONTENT\n",
      "Length: 205487817 (196M), 170574622 (163M) remaining [text/plain]\n",
      "Saving to: ‘natural_disasters_BIG.csv’\n",
      "\n",
      "sasters_BIG.csv      61%[+++========>        ] 120.60M  62.6KB/s    eta 8m 23s "
     ]
    }
   ],
   "source": [
    "!wget -O natural_disasters_BIG.csv https://www.dropbox.com/s/b74bpkbr89l8k43/natural_disasters_BIG.csv?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How many disasters occurred in continent C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file dataframe_1.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('disasters').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    # import input CSV file\n",
    "    lines = sc.textFile('natural_disasters_BIG.csv')\n",
    "    # separate each line by its columns and retrieve only the columns with continents and occurrences\n",
    "    disasterRows = lines.map( lambda line : line.split(',') ) \\\n",
    "                        .map( lambda arr : Row( continent = arr[5], count=int(arr[9])))\n",
    "    # create a data frame with the continent and occurrences columns\n",
    "    disasterRowsDF = spark.createDataFrame( disasterRows )\n",
    "    \n",
    "    # group the data by continents and, for each continent, sum the occurrences count and aggregate this new column\n",
    "    # to the column with the continents\n",
    "    continentsDF = disasterRowsDF.groupBy('continent').agg(sum('count').alias('count'))\n",
    "    \n",
    "    # show all results\n",
    "    continentsDF.show()\n",
    "    sc.stop()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash -c \"time python dataframe_1.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. In which regions there were disasters of type X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file dataframe_2.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('disasters').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    # import input CSV file\n",
    "    lines = sc.textFile('natural_disasters_BIG.csv')\n",
    "    # separate each line by its columns and retrieve only the columns with disaster type and region\n",
    "    disasterRows = lines.map( lambda line : line.split(',') ) \\\n",
    "                        .map( lambda arr : Row( type_x = arr[3], region = arr[6]))\n",
    "    # create a data frame with the disaster type and regions columns\n",
    "    disasterRowsDF = spark.createDataFrame( disasterRows )\n",
    "    \n",
    "    # group the data by disaster type and, for each type, collect the distinct regions in a list\n",
    "    regionByType = disasterRowsDF.groupBy('type_x').agg(collect_set('region').alias('region'))\n",
    "    \n",
    "    # show results\n",
    "    regionByType.show()\n",
    "    sc.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash -c \"time python dataframe_2.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What are the probabilities of getting injured or dying in a natural disaster of type T in the continent C during decade D (190x, 191x, 192x, ..., 199x, 200x, 201x)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file dataframe_3.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('disasters').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    # import input CSV file\n",
    "    lines = sc.textFile('natural_disasters_BIG.csv')\n",
    "    # separate each line by its columns, remove the empty lines in the columns of interest and retrieve only the columns \n",
    "    # with the decade (year column with a 0 in the last digit), the disaster type, the continent, the deaths, the injured \n",
    "    # and the total affected\n",
    "    disasterRows = lines.map( lambda line : line.split(',') ) \\\n",
    "                        .filter(lambda lines: len(lines[3])>0 and len(lines[5]) and len(lines[10])>0 and len(lines[11])>0 and len(lines[14])>0) \\\n",
    "                        .map( lambda arr : Row(decade = arr[0][:len(arr[0])-1]+\"0\", type_ = arr[3], continent = arr[5],\n",
    "                                               deaths = int(arr[10]), injured = int(arr[11]), affected = int(arr[14])))\n",
    "    # create a data frame with the selected columns\n",
    "    disasterRowsDF = spark.createDataFrame( disasterRows )\n",
    "    \n",
    "    # group the data by decade, disaster type and continent and, for each combination and that keys, sum the numbers of\n",
    "    # deaths, the number of injured and the number of total affected\n",
    "    summed_stats_columns = disasterRowsDF.groupBy('decade','type_','continent') \\\n",
    "                         .agg(sum('deaths').alias('deaths'),sum('injured').alias('injured'),sum('affected').alias('affected'))\n",
    "    \n",
    "    # create a new column with the summed deaths divided by the summed total affected\n",
    "    death_probs = summed_stats_columns.withColumn('death_prob', summed_stats_columns['deaths'] / summed_stats_columns['affected'])\n",
    "    \n",
    "    # create a new column with the summed injured divided by the summed total affected\n",
    "    injured_probs = death_probs.withColumn('injury_prob', summed_stats_columns['injured'] / summed_stats_columns['affected'])\n",
    "    \n",
    "    # drop the original columns with the summed deaths, injured and total affected\n",
    "    final_result = injured_probs.drop(\"deaths\").drop(\"injured\").drop(\"affected\")\n",
    "    \n",
    "    # show results\n",
    "    final_result.show()\n",
    "    sc.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash -c \"time python dataframe_3.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Optional Exercise. What is the mean total damage and most costly disaster's expanses of a certain disaster subgroup in each country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file dataframe_4.py\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('disasters').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    # import input CSV file\n",
    "    lines = sc.textFile('natural_disasters_BIG.csv')\n",
    "    # separate each line by its columns, remove the empty lines in the column of interest and retrieve only the columns \n",
    "    # with the country, disaster subgroup, total damage cost and ocurrences\n",
    "    disasterRows = lines.map( lambda line : line.split(',') ) \\\n",
    "                        .filter(lambda lines: len(lines[15])>0) \\\n",
    "                        .map( lambda arr : Row(country = arr[8], subgroup = arr[2], total_damage = float(arr[15]), occurrences = int(arr[9])))\n",
    "    # create a data frame with the selected columns\n",
    "    disasterRowsDF = spark.createDataFrame( disasterRows )\n",
    "    \n",
    "    # group the data by disaster subgroup and country and, for each combination and those keys, sum the numbers of\n",
    "    # total damages and get the maximum for total damage\n",
    "    summed_stats_columns = disasterRowsDF.groupBy('country','subgroup') \\\n",
    "                         .agg(sum('total_damage').alias('total_damage'), sum('occurrences').alias('occurrences'),max('total_damage').alias('max_damage'))\n",
    "    \n",
    "    # create a new column with the summed total_damage divided by the occurrences\n",
    "    mean_damage = summed_stats_columns.withColumn('mean_damage', summed_stats_columns['total_damage'] / summed_stats_columns['occurrences'])\n",
    "    \n",
    "    # drop the original columns with the summed total damages and occurrences\n",
    "    final_result = mean_damage.drop(\"total_damage\").drop(\"occurrences\")\n",
    "    \n",
    "    # show results\n",
    "    final_result.show()\n",
    "    sc.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash -c \"time python dataframe_4.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
